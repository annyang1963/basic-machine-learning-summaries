{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles\n",
    "* bagging\n",
    "* adaptive boosting\n",
    "* gradient boosting\n",
    "* extreme gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "In bagging technique, many **independent** predictors/models/learners are combined using some model averaging techniques. (e.g. weighted average, majority vote or normal average)\n",
    "\n",
    "It bootstrap data for each learner, so that all the models are little different from each other. Each observation is chosen **with replacement** to be used as input for each of the model. So, each model will have different observations based on the bootstrap process. Because this technique takes many **uncorrelated learners** to make a final model, it reduces error by **reducing variance**. Example of bagging ensemble is Random Forest models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "**Subsequent learners learn from the mistakes of the previous learner.** Therefore, the observations have an **unequal probability** of appearing in subsequent models and ones with the **highest error appear most**. (The observations are not chosen based on the bootstrap process, but based on the error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://miro.medium.com/max/2848/1*PaXJ8HCYE9r2MgiZ32TQ2A.png width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://miro.medium.com/max/2552/1*8T4HEjzHto_V8PrEFLkd9A.png width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive boosting algorithm\n",
    "Each observation is initially assigned an equal weight. After evaluating the first learner, we **increase the weights of those observations that are difficult to classify** (i.e. those are predicted wrong by previous learners.) and lower the weights for those that are easy to classify. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a data set containing n points, where\n",
    "<img src=https://miro.medium.com/max/470/1*2fp-O3KfXqrdYEGU_RjY0w.jpeg>\n",
    "Initialize the weight for each data point as:\n",
    "<img src=https://miro.medium.com/max/546/1*IMHTVrXPKc2mVqDDK40k9w.jpeg>\n",
    "For iteration m=1,…,M:\n",
    "\n",
    "(1) Fit weak classifiers to the data set and select the one with the lowest weighted classification error:\n",
    "<img src=https://miro.medium.com/max/388/1*C8-yNia8Oh44X-t0UxUCUA.jpeg>\n",
    "(2) Calculate the weight for the m_th weak classifier:\n",
    "<img src=https://miro.medium.com/max/412/1*jFpUGuxpGZuzpG6FlDAASw.jpeg>\n",
    "We only don’t want any classifier with exact 50% accuracy, which doesn’t add any information and thus contributes nothing to the final prediction.\n",
    "\n",
    "(3) Update the weight for each data point as:\n",
    "<img src=https://miro.medium.com/max/930/1*mqLcX8yookiPVZoAe6iwqA.jpeg>\n",
    "**The “exp” term in the numerator makes missclassified data have larger weight.**\n",
    "For a missclassified observation, the “exp” term would be always larger than 1 (y*f is always -1, theta_m is positive). Thus misclassified cases would be updated with larger weights after an iteration. \n",
    "\n",
    "(4) After M iteration we can get the final prediction by summing up the weighted prediction of each classifier.\n",
    "<img src=https://miro.medium.com/max/588/1*B2987FKIw3QL2ClYR_OeuQ.jpeg>\n",
    "where f_m stands for the m_th weak classifier and theta_m is the corresponding weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General boosing algorithm\n",
    "Boosting algorithm fits the ensemble models of the kind\n",
    "<img src=https://miro.medium.com/max/900/1*ERegahgd879qqEBxo8Vleg.jpeg>\n",
    "where f0 is the initial guess, ϕm(x) is the base estimator at iteration m and θm is the weight for the m_th estimator. The product θmϕm(x) denotes the “step” at iteration m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most boosting algorithms can be viewed as to solve for the minimum loss function.**\n",
    "\n",
    "For **AdaBoost**, it solves this equation for the **exponential loss function** under the constraint that ϕm(x) only outputs -1 or 1. GBM and Xgboost use different loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosing algorithm\n",
    "Gradient Boosting trains many models in a gradual, additive and sequential manner. The major difference between AdaBoost and Gradient Boosting Algorithm is **how the two algorithms identify the shortcomings of weak learners**\n",
    "\n",
    "While the AdaBoost model identifies the shortcomings by using **high weight data points**, gradient boosting performs the same by using gradients in the loss function, **i.e.updating the model by adding learners that decrease loss function using gradient descent,**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Similar to gradient descent in parameter space, at the m_th iteration, the direction of the steepest descent is given by the **negative gradient of the loss function**:\n",
    "<img src=https://miro.medium.com/max/824/1*EUIz2s4aEqOR9a0pirNiiQ.png>\n",
    "Taking a step along this direction is guaranteed to reduce the loss. Then determine the step length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration, a model is fitted to predict the negative gradient. Typically the squared error is used as a surrogate loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting (Xgboost)\n",
    "GBM divides the optimization problem into two parts by first determining the direction of the step and then optimizing the step length. XGBoost tries to determine the step **directly by solving**\n",
    "<img src=https://miro.medium.com/max/634/1*QAIReONJ8r6AmHKuaVotdQ.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each x in the data set. By doing second-order Taylor expansion of the loss function around the current estimate f(m-1)(x), we get\n",
    "<img src=https://miro.medium.com/max/1062/1*WWInCZCh7KhQmi8nscJMYw.png>\n",
    "where g_m(x) is the gradient, same as the one in GBM, and h_m(x) is the Hessian (second order derivative) at the current estimate:\n",
    "<img src=https://miro.medium.com/max/742/1*qYj8oeFqvmAc5X8a66C7uQ.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that XGBoost provides variety of **regularization** to improve generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
