{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM vs XGBoost vs CatBoost\n",
    "* Structural Differences\n",
    "* Treatment of categorical variables by each algorithm\n",
    "* Performance of each algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural Differences\n",
    "LightGBM uses a novel technique of **Gradient-based One-Side Sampling (GOSS)** to filter out the data instances for finding a split value while XGBoost uses **pre-sorted algorithm & Histogram-based algorithm** for computing the best split. Here instances mean observations/samples.\n",
    "\n",
    "### pre-sorted splitting\n",
    "For each node, enumerate over all features\n",
    "\n",
    "For each feature, sort the instances by feature value\n",
    "\n",
    "Use a linear scan to decide the best split along that feature basis **information gain**\n",
    "\n",
    "Take the best split solution along all the features\n",
    "\n",
    "### Histogram-based algorithm \n",
    "\n",
    "splits all the data points for a feature into **discrete bins** and uses these bins to find the split value of histogram. While, it is efficient than pre-sorted algorithm in training speed, it is still behind GOSS in terms of speed.\n",
    "\n",
    "### GOSS\n",
    "Logically if gradient of a data points is large in some sense, these points are important for finding the optimal split point as they have higher error.\n",
    "\n",
    "GOSS keeps all the instances with large gradients and performs random sampling on the instances with small gradients. For example, let’s say I have 500K rows of data where 10k rows have higher gradients. So my algorithm will choose (10k rows of higher gradient+ x% of remaining 490k rows chosen randomly). Assuming x is 10%, total rows selected are 59k out of 500K on the basis of which split value if found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment of categorical variables by each algorithm\n",
    "### CatBoost\n",
    "CatBoost has the flexibility of giving indices of categorical columns so that it can be encoded as **one-hot encoding using one_hot_max_size** (Use one-hot encoding for all features with number of different values less than or equal to the given parameter value).\n",
    "If you don’t pass any anything in cat_features argument, CatBoost will treat all the columns as numerical variables.\n",
    "\n",
    "If a column having string values is not provided in the cat_features, CatBoost throws an error. Also, a column having default int type will be treated as numeric by default, one has to specify it in cat_features to make the algorithm treat it as categorical.\n",
    "\n",
    "### LightGBM\n",
    "Similar to CatBoost, LightGBM can also handle categorical features by taking the input of feature names. It does not convert to one-hot coding, and is much faster than one-hot coding. LGBM uses a special algorithm to find the split value of categorical features.\n",
    "\n",
    "You should convert your categorical features to **int type** before you construct Dataset for LGBM. It does not accept string values even if you passes it through categorical_feature parameter.\n",
    "\n",
    "### XGBoost\n",
    "Unlike CatBoost or LGBM, XGBoost cannot handle categorical features by itself, it only accepts **numerical values** similar to Random Forest. Therefore one has to perform various encodings like label encoding, mean encoding or one-hot encoding before supplying categorical data to XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of each algorithm\n",
    "### CatBoost\n",
    "CatBoost performs well only when we have categorical variables in the data and we properly tune them.\n",
    "\n",
    "### XGBoost\n",
    "XGBoost which generally works well. But it is too slow.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\n",
    "\n",
    "### LightGBM\n",
    "faster than XGBoost.\n",
    "\n",
    "LightGBM grow trees in leaf wise but other boosting method grow tress in level wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’.\n",
    " \n",
    "Leaf wise splits lead to increase in complexity and may lead to overfitting and it can be overcome by specifying another parameter max-depth which specifies the depth to which splitting will occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBDT vs CNN vs SVM\n",
    "* SVM\n",
    "\n",
    "As a rule of thumb, I'd say that SVMs are great for **relatively small data sets with fewer outliers**. There are a lot of knobs to be turned in SVMs: Choosing the **\"right\" kernel, regularization penalties...** We can end up with **a lot of support vectors** in SVMs; in the worst-case scenario, we have as many support vectors as we have samples in the training set.\n",
    "Although, there are multi-class SVMs, the typical implementation for mult-class classification is **One-vs.-All**; thus, we have to train an SVM for each class which is not efficient-- in contrast, decision trees or random forests, which can handle multiple classes out of the box.\n",
    "\n",
    "* RF\n",
    "\n",
    "Random forests may **require more data** but they almost always come up with a pretty robust model. But it is a **non-parametric model** which can thus be more expensive, computationally, compared to a generalized linear model. The complexity of a random forest grows with the number of trees in the forest, and the number of training samples we have.\n",
    "\n",
    "* NN\n",
    "\n",
    "And deep learning algorithms... well, they require **\"relatively\" large datasets** to work well, and you also need the **infrastructure to train them in reasonable time**. Also, deep learning algorithms **require much more experience**: Setting up a neural network using deep learning algorithms is much more tedious than using an off-the-shelf classifiers such as random forests and SVMs. On the other hand, deep learning really shines when it comes to **complex problems such as image classification, natural language processing, and speech recognition**. Another advantage is that you have to **worry less about the feature engineering part**.\n",
    "\n",
    "* high dimensional raw data and feature extraction\n",
    "\n",
    "neural networks are very efficient for dealing with **high dimensional raw data**. Image, video, text and audio are all examples of high dimensional raw data that is **very hard to preprocess and represent as features**. \n",
    "\n",
    "* missing values\n",
    "\n",
    "If you ever tried to feed a neural network with a missing data you probably ended with errors. This is because the **equations which are being solved during NN training assume a valid value for each input variable.** XGBoost on the other hand, has its own way of dealing with missing data. \n",
    "\n",
    "* training time and efficiency\n",
    "\n",
    "Neural networks training is “embarrassingly parallel”, making them **great for parallel and distributed training**. That is, if your budget can cover running hours of training on expensive machines with TPUs or GPUs.\n",
    "\n",
    "On the other hand, if you only use up to several million of records XGBoost can be trained on a **less expensive multi-core CPU and converge in less time**. So if you have a limited amount of data and want to train a model — XGBoost may be more affordable and achieve similar results.\n",
    "\n",
    "* how munch data\n",
    "\n",
    "Due to its underlying data structure, XGBoost is limited in the ways **it can be parallelized making it short in the amount of data it can process**. One way of handling massive datasets is **splitting the data to shards and stacking models** — thus effectively multiplying the number of parameters used to fit the data.\n",
    "\n",
    "However with neural networks it is usually **“the more the merrier”**. When dealing with massive datasets neural networks can converge with the same number of parameters to lower generalization error. But for smaller datasets XBGoost typically converges faster and with smaller error.\n",
    "\n",
    "* how complex the input and output\n",
    "\n",
    "XGBoost has more limitations than NNs regarding the shape of the data it can work with. It usually take **1-d arrays as record inputs and outputs a single number (regression) or a vector of probabilities (classification)**. For this reason, it is easier to configure an XGBoost pipeline. In XGBoost there is **no need to worry about shapes of data** — just provide a pandas datafame that looks like a table, set the label column and you are good to go.\n",
    "\n",
    "Neural networks on the other hand, are designed to **work on tensors — a high dimensional matrix**. NN’s output and input shape can vary between numbers, sequences (vectors), images and even videos. So for classic problems like click-though prediction based on structured data —both can work well. in terms of data shape. But when it comes building **more complex data transformations** — the NN’s may be your only valid choise!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
